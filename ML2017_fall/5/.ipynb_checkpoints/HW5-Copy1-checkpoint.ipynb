{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "# copy-paste of seminar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS='floatX=float32'\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_shape = (8,)  n_actions = 4\n"
     ]
    }
   ],
   "source": [
    "# load game and look at it\n",
    "\n",
    "import gym\n",
    "make_env = lambda: gym.make(\"LunarLander-v2\")\n",
    "\n",
    "env=make_env()\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"state_shape =\", state_shape, \" n_actions =\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXpI69f8B0ierrJwKScIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7aac50e7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import elu\n",
    "\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,)+state_shape)\n",
    "\n",
    "\n",
    "nn = DenseLayer(observation_layer, 60, nonlinearity=elu)\n",
    "nn = DenseLayer(nn, 60, nonlinearity=elu)\n",
    "nn = DenseLayer(nn, 40, nonlinearity=elu)\n",
    "nn = DenseLayer(nn, 20, nonlinearity=elu)\n",
    "\n",
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(nn,num_units=n_actions,\n",
    "                           nonlinearity=None,name=\"q-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking actions is done by yet another layer, that implements $ \\epsilon$ -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)\n",
    "\n",
    "#set starting epsilon\n",
    "action_layer.epsilon.set_value(np.float32(0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We define an agent entirely composed of a lasagne network:\n",
    "* Observations as InputLayer(s)\n",
    "* Actions as intermediate Layer(s)\n",
    "* `policy_estimators` is \"whatever else you want to keep track of\"\n",
    "\n",
    "Each parameter can be either one layer or a list of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              action_layers=action_layer,\n",
    "              policy_estimators=qvalues_layer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, W, b, q-values.W, q-values.b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,make_env,n_games=1,max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: [[1 1 3 1 0]]\n",
      "rewards: [[-1.1994961  -1.56783587  0.45341898 -1.51351082  0.        ]]\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "obs_log,action_log,reward_log,_,_,_  = pool.interact(5)\n",
    "\n",
    "\n",
    "print('actions:',action_log)\n",
    "print('rewards:',reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll train on rollouts of 10 steps (required by n-step algorithms and rnns later)\n",
    "SEQ_LENGTH=10\n",
    "\n",
    "#load first sessions (this function calls interact and stores sessions in the pool)\n",
    "\n",
    "for _ in range(100):\n",
    "    pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "\n",
    "We shall now define a function that replays recent game sessions and updates network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2, like you implemented before in lasagne.\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,\n",
    "                                                      n_steps=1,)\n",
    "\n",
    "#compute mean loss over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get weight updates\n",
    "updates = lasagne.updates.adam(loss,weights)\n",
    "\n",
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run\n",
    "\n",
    "Play full session with an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "#untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "# from IPython.display import HTML\n",
    "# import os\n",
    "\n",
    "# video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "# HTML(\"\"\"\n",
    "# <video width=\"640\" height=\"480\" controls>\n",
    "#   <source src=\"{}\" type=\"video/mp4\">\n",
    "# </video>\n",
    "# \"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 1 #starting epoch\n",
    "rewards = {} #full game rewards\n",
    "#target_score = -90\n",
    "target_score = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 97/3000 [00:04<02:18, 20.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 114 timesteps with reward=-493.176894724768\n",
      "Episode finished after 102 timesteps with reward=-460.9381116105444\n",
      "Episode finished after 66 timesteps with reward=-271.910406127584\n",
      "Episode finished after 94 timesteps with reward=-116.11929543450213\n",
      "Episode finished after 106 timesteps with reward=-169.3217698088548\n",
      "Episode finished after 78 timesteps with reward=-370.13753864715613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 102/3000 [00:05<02:30, 19.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 127 timesteps with reward=-190.43116005563155\n",
      "Episode finished after 92 timesteps with reward=-352.1972006179297\n",
      "Episode finished after 73 timesteps with reward=-126.0521383612176\n",
      "Episode finished after 98 timesteps with reward=-463.3761835099949\n",
      "iter=100\tepsilon=0.910\n",
      "-301.36606989\n",
      "Current score(mean over 10) = -301.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 198/3000 [00:10<02:25, 19.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 123 timesteps with reward=-444.21962169305993\n",
      "Episode finished after 88 timesteps with reward=-160.49734580660854\n",
      "Episode finished after 92 timesteps with reward=-323.75031741252343\n",
      "Episode finished after 94 timesteps with reward=-301.92629257292185\n",
      "Episode finished after 111 timesteps with reward=-281.4340093703771\n",
      "Episode finished after 135 timesteps with reward=-113.28205542878206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 202/3000 [00:10<02:32, 18.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 98 timesteps with reward=-444.8230573159086\n",
      "Episode finished after 94 timesteps with reward=-98.68710002933055\n",
      "Episode finished after 78 timesteps with reward=-141.10487339395544\n",
      "Episode finished after 134 timesteps with reward=-103.43632833310005\n",
      "iter=200\tepsilon=0.828\n",
      "-241.316100136\n",
      "Current score(mean over 10) = -241.316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 298/3000 [00:16<02:26, 18.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 108 timesteps with reward=-64.32342104375843\n",
      "Episode finished after 68 timesteps with reward=-134.34180190843344\n",
      "Episode finished after 100 timesteps with reward=-80.47050208619282\n",
      "Episode finished after 137 timesteps with reward=-139.17655377851355\n",
      "Episode finished after 83 timesteps with reward=-150.72726614159063\n",
      "Episode finished after 70 timesteps with reward=-143.4523576716821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 302/3000 [00:16<02:30, 17.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 91 timesteps with reward=-129.71457339404589\n",
      "Episode finished after 86 timesteps with reward=-140.88541487834445\n",
      "Episode finished after 165 timesteps with reward=-86.58369844455248\n",
      "Episode finished after 91 timesteps with reward=-128.12058255864147\n",
      "iter=300\tepsilon=0.754\n",
      "-119.779617191\n",
      "Current score(mean over 10) = -119.780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 399/3000 [00:23<02:34, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 158 timesteps with reward=-177.94797923449457\n",
      "Episode finished after 117 timesteps with reward=-65.8731369221232\n",
      "Episode finished after 114 timesteps with reward=-159.48167613584133\n",
      "Episode finished after 111 timesteps with reward=-78.55046240143302\n",
      "Episode finished after 136 timesteps with reward=-188.71971007295235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 401/3000 [00:24<02:38, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 161 timesteps with reward=-110.58498408246206\n",
      "Episode finished after 95 timesteps with reward=-162.66461545213056\n",
      "Episode finished after 162 timesteps with reward=-151.86539882524332\n",
      "Episode finished after 75 timesteps with reward=-64.434646160686\n",
      "Episode finished after 98 timesteps with reward=-84.06978255247434\n",
      "iter=400\tepsilon=0.687\n",
      "-124.419239184\n",
      "Current score(mean over 10) = -124.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 498/3000 [00:29<02:28, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 151 timesteps with reward=-62.36698247488113\n",
      "Episode finished after 145 timesteps with reward=-182.01439693433218\n",
      "Episode finished after 87 timesteps with reward=-164.63471185198398\n",
      "Episode finished after 229 timesteps with reward=-199.31449946686578\n",
      "Episode finished after 128 timesteps with reward=-164.89171658997176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 501/3000 [00:30<02:31, 16.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 125 timesteps with reward=-216.52351347810188\n",
      "Episode finished after 137 timesteps with reward=-200.21916569730172\n",
      "Episode finished after 116 timesteps with reward=-205.60695572263367\n",
      "Episode finished after 125 timesteps with reward=-136.23200547721726\n",
      "Episode finished after 187 timesteps with reward=-200.58276578436798\n",
      "iter=500\tepsilon=0.626\n",
      "-173.238671348\n",
      "Current score(mean over 10) = -173.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 599/3000 [00:35<02:23, 16.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 153 timesteps with reward=-42.579881737219964\n",
      "Episode finished after 102 timesteps with reward=-145.57220803775803\n",
      "Episode finished after 195 timesteps with reward=-76.71251982299682\n",
      "Episode finished after 95 timesteps with reward=-185.2051054764821\n",
      "Episode finished after 147 timesteps with reward=-160.77799289986422\n",
      "Episode finished after 97 timesteps with reward=-135.63187832682033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 601/3000 [00:36<02:25, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 142 timesteps with reward=-177.19340503157582\n",
      "Episode finished after 155 timesteps with reward=-136.87296496073586\n",
      "Episode finished after 82 timesteps with reward=-103.5597532015105\n",
      "Episode finished after 211 timesteps with reward=-92.29790388604229\n",
      "iter=600\tepsilon=0.571\n",
      "-125.640361338\n",
      "Current score(mean over 10) = -125.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 698/3000 [00:41<02:18, 16.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 143 timesteps with reward=-56.52179367813228\n",
      "Episode finished after 91 timesteps with reward=-220.1358957435343\n",
      "Episode finished after 123 timesteps with reward=-130.03740807220206\n",
      "Episode finished after 114 timesteps with reward=-68.26745340865926\n",
      "Episode finished after 97 timesteps with reward=-211.50634459197903\n",
      "Episode finished after 154 timesteps with reward=-161.19468011917044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 702/3000 [00:42<02:19, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 185 timesteps with reward=-159.18059861099528\n",
      "Episode finished after 196 timesteps with reward=-139.35138136573443\n",
      "Episode finished after 211 timesteps with reward=-147.39993393216517\n",
      "Episode finished after 200 timesteps with reward=-97.04225632376009\n",
      "iter=700\tepsilon=0.522\n",
      "-139.063774585\n",
      "Current score(mean over 10) = -139.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 798/3000 [00:49<02:17, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 203 timesteps with reward=-191.93897425263268\n",
      "Episode finished after 1000 timesteps with reward=46.47830008343589\n",
      "Episode finished after 243 timesteps with reward=-84.56351744451644\n",
      "Episode finished after 134 timesteps with reward=-153.84895032771306\n",
      "Episode finished after 101 timesteps with reward=-100.31287043345152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 800/3000 [00:51<02:21, 15.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 136 timesteps with reward=-156.76990467677416\n",
      "Episode finished after 267 timesteps with reward=-108.79077839887069\n",
      "Episode finished after 174 timesteps with reward=-96.04085284131384\n",
      "Episode finished after 119 timesteps with reward=-201.96639742635006\n",
      "Episode finished after 134 timesteps with reward=-179.8028014944145\n",
      "iter=800\tepsilon=0.477\n",
      "-122.755674721\n",
      "Current score(mean over 10) = -122.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 898/3000 [00:57<02:13, 15.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 234 timesteps with reward=-65.45257613283799\n",
      "Episode finished after 1000 timesteps with reward=-51.6810767717494\n",
      "Episode finished after 209 timesteps with reward=-140.75262800839332\n",
      "Episode finished after 1000 timesteps with reward=57.53232326991088\n",
      "Episode finished after 253 timesteps with reward=-72.13924338895342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 900/3000 [01:00<02:21, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 242 timesteps with reward=-17.49817119659663\n",
      "Episode finished after 225 timesteps with reward=-58.34691290201371\n",
      "Episode finished after 288 timesteps with reward=-45.96752543361772\n",
      "Episode finished after 179 timesteps with reward=-22.75597186759792\n",
      "Episode finished after 258 timesteps with reward=-201.3185619027371\n",
      "iter=900\tepsilon=0.436\n",
      "-61.8380344335\n",
      "Current score(mean over 10) = -61.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 998/3000 [01:08<02:17, 14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=8.680892748431614\n",
      "Episode finished after 1000 timesteps with reward=17.812559630132778\n",
      "Episode finished after 121 timesteps with reward=-218.6004149021943\n",
      "Episode finished after 1000 timesteps with reward=77.4446018270453\n",
      "Episode finished after 1000 timesteps with reward=42.59395780208051\n",
      "Episode finished after 327 timesteps with reward=-74.60515147494314\n",
      "Episode finished after 324 timesteps with reward=-42.19077072460334\n",
      "Episode finished after 1000 timesteps with reward=44.579207456527676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1002/3000 [01:15<02:31, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=46.12808996838541\n",
      "Episode finished after 183 timesteps with reward=-31.483396116935978\n",
      "iter=1000\tepsilon=0.399\n",
      "-12.9640423786\n",
      "Current score(mean over 10) = -12.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1098/3000 [01:24<02:26, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 312 timesteps with reward=-16.72653197347978\n",
      "Episode finished after 407 timesteps with reward=-33.22534592801736\n",
      "Episode finished after 201 timesteps with reward=-94.73679181121734\n",
      "Episode finished after 1000 timesteps with reward=40.16015847370937\n",
      "Episode finished after 1000 timesteps with reward=82.57414024958071\n",
      "Episode finished after 1000 timesteps with reward=68.30956201219648\n",
      "Episode finished after 1000 timesteps with reward=79.21514199826456\n",
      "Episode finished after 1000 timesteps with reward=8.523073857628638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1102/3000 [01:31<02:38, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=43.899751960546254\n",
      "Episode finished after 180 timesteps with reward=-37.540118836201216\n",
      "iter=1100\tepsilon=0.366\n",
      "14.0453040003\n",
      "Current score(mean over 10) = 14.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 1198/3000 [01:40<02:30, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 262 timesteps with reward=-86.8677555468449\n",
      "Episode finished after 829 timesteps with reward=111.78278253730281\n",
      "Episode finished after 1000 timesteps with reward=-23.304545760219753\n",
      "Episode finished after 166 timesteps with reward=-124.5167746344027\n",
      "Episode finished after 205 timesteps with reward=-112.67067807757232\n",
      "Episode finished after 402 timesteps with reward=177.60969441356238\n",
      "Episode finished after 752 timesteps with reward=202.16712714071267\n",
      "Episode finished after 1000 timesteps with reward=87.63768024946617\n",
      "Episode finished after 259 timesteps with reward=-6.7855028807187665"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1201/3000 [01:46<02:39, 11.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 1000 timesteps with reward=-16.396505570046045\n",
      "iter=1200\tepsilon=0.336\n",
      "20.8655521871\n",
      "Current score(mean over 10) = 20.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1298/3000 [01:57<02:34, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 522 timesteps with reward=193.89881409217833\n",
      "Episode finished after 1000 timesteps with reward=95.78703001474315\n",
      "Episode finished after 516 timesteps with reward=168.72584285627244\n",
      "Episode finished after 1000 timesteps with reward=47.3237551327307\n",
      "Episode finished after 1000 timesteps with reward=91.22636518814168\n",
      "Episode finished after 1000 timesteps with reward=50.34795264775525\n",
      "Episode finished after 1000 timesteps with reward=58.181037940800465\n",
      "Episode finished after 1000 timesteps with reward=22.530953734543438\n",
      "Episode finished after 907 timesteps with reward=177.5895991658084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1301/3000 [02:10<02:50,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=66.97870512139092\n",
      "iter=1300\tepsilon=0.309\n",
      "97.2590055894\n",
      "Current score(mean over 10) = 97.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1399/3000 [02:22<02:42,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=8.112182523038243\n",
      "Episode finished after 1000 timesteps with reward=3.549493010427677\n",
      "Episode finished after 1000 timesteps with reward=-7.7620362925674256\n",
      "Episode finished after 1000 timesteps with reward=21.498399721923533\n",
      "Episode finished after 1000 timesteps with reward=71.79908918257988\n",
      "Episode finished after 1000 timesteps with reward=34.54974531418115\n",
      "Episode finished after 1000 timesteps with reward=42.859737694452605\n",
      "Episode finished after 1000 timesteps with reward=-45.54485156783333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 1400/3000 [02:36<02:58,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=83.23838993059174\n",
      "Episode finished after 966 timesteps with reward=102.50616753064544\n",
      "iter=1400\tepsilon=0.284\n",
      "31.4806317047\n",
      "Current score(mean over 10) = 31.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 1498/3000 [02:45<02:46,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=58.610397568678245\n",
      "Episode finished after 735 timesteps with reward=140.76249744969036\n",
      "Episode finished after 1000 timesteps with reward=-26.539666637026414\n",
      "Episode finished after 1000 timesteps with reward=-111.33612774312573\n",
      "Episode finished after 360 timesteps with reward=-67.4616534843523\n",
      "Episode finished after 1000 timesteps with reward=40.872765240118696\n",
      "Episode finished after 1000 timesteps with reward=-102.86356854262795\n",
      "Episode finished after 393 timesteps with reward=-375.13870283129364\n",
      "Episode finished after 1000 timesteps with reward=-142.71130401557093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1501/3000 [02:55<02:55,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-135.04731394694159\n",
      "iter=1500\tepsilon=0.262\n",
      "-72.0852676942\n",
      "Current score(mean over 10) = -72.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1598/3000 [03:04<02:41,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-27.82002688443606\n",
      "Episode finished after 1000 timesteps with reward=11.806678852079106\n",
      "Episode finished after 1000 timesteps with reward=36.738420108825586\n",
      "Episode finished after 1000 timesteps with reward=105.09653283566084\n",
      "Episode finished after 513 timesteps with reward=150.1717710567611\n",
      "Episode finished after 1000 timesteps with reward=43.96265304430274\n",
      "Episode finished after 1000 timesteps with reward=24.17181724069236\n",
      "Episode finished after 1000 timesteps with reward=4.2593745658820055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1601/3000 [03:14<02:50,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-7.2001543843702445\n",
      "Episode finished after 1000 timesteps with reward=-34.03157965717931\n",
      "iter=1600\tepsilon=0.242\n",
      "30.7155486778\n",
      "Current score(mean over 10) = 30.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1699/3000 [03:24<02:36,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 832 timesteps with reward=104.47574547996084\n",
      "Episode finished after 1000 timesteps with reward=-40.14245183456292\n",
      "Episode finished after 839 timesteps with reward=-250.2474369850925\n",
      "Episode finished after 1000 timesteps with reward=-34.55141444126338\n",
      "Episode finished after 1000 timesteps with reward=-38.01141104223664\n",
      "Episode finished after 1000 timesteps with reward=-24.4808142470435\n",
      "Episode finished after 1000 timesteps with reward=-56.57444465321095\n",
      "Episode finished after 538 timesteps with reward=-39.51620672989351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 1701/3000 [03:36<02:45,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-40.017203239717745\n",
      "Episode finished after 1000 timesteps with reward=-134.59562987325887\n",
      "iter=1700\tepsilon=0.224\n",
      "-55.3661267566\n",
      "Current score(mean over 10) = -55.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 1799/3000 [03:48<02:32,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 866 timesteps with reward=169.324423541846\n",
      "Episode finished after 1000 timesteps with reward=32.01878630416651\n",
      "Episode finished after 1000 timesteps with reward=59.55044568819598\n",
      "Episode finished after 915 timesteps with reward=143.5798441987687\n",
      "Episode finished after 1000 timesteps with reward=80.86965156599832\n",
      "Episode finished after 1000 timesteps with reward=19.066594653075203\n",
      "Episode finished after 629 timesteps with reward=174.38524073568473\n",
      "Episode finished after 1000 timesteps with reward=-153.4820935819102\n",
      "Episode finished after 1000 timesteps with reward=42.50307263270847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1801/3000 [04:02<02:41,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=56.27669452331248\n",
      "iter=1800\tepsilon=0.207\n",
      "62.4092660262\n",
      "Current score(mean over 10) = 62.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1899/3000 [04:13<02:26,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-74.00379097792916\n",
      "Episode finished after 1000 timesteps with reward=-105.9932931151806\n",
      "Episode finished after 328 timesteps with reward=-46.20722261018298\n",
      "Episode finished after 1000 timesteps with reward=-75.88778004569134\n",
      "Episode finished after 1000 timesteps with reward=-75.46772980386365\n",
      "Episode finished after 1000 timesteps with reward=-56.48100416440938\n",
      "Episode finished after 1000 timesteps with reward=-82.54270599965312\n",
      "Episode finished after 1000 timesteps with reward=-112.22988454657246\n",
      "Episode finished after 1000 timesteps with reward=-93.90979215281614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1901/3000 [04:27<02:34,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-98.88260124661056\n",
      "iter=1900\tepsilon=0.192\n",
      "-82.1605804663\n",
      "Current score(mean over 10) = -82.161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1999/3000 [04:38<02:19,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-102.66172154554698\n",
      "Episode finished after 1000 timesteps with reward=12.892213420468591\n",
      "Episode finished after 1000 timesteps with reward=-83.73278141858367\n",
      "Episode finished after 1000 timesteps with reward=-61.542206666855094\n",
      "Episode finished after 1000 timesteps with reward=-79.2948546168537\n",
      "Episode finished after 484 timesteps with reward=215.8781832107138\n",
      "Episode finished after 951 timesteps with reward=131.96308283416676\n",
      "Episode finished after 708 timesteps with reward=-481.37879888630573\n",
      "Episode finished after 1000 timesteps with reward=-72.26981720721342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2001/3000 [04:49<02:24,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=27.862837996981796\n",
      "iter=2000\tepsilon=0.179\n",
      "-49.2283862879\n",
      "Current score(mean over 10) = -49.228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 2099/3000 [05:02<02:09,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-181.83138221268254\n",
      "Episode finished after 1000 timesteps with reward=8.652914867975618\n",
      "Episode finished after 568 timesteps with reward=-157.27310638051722\n",
      "Episode finished after 680 timesteps with reward=129.36580850938697\n",
      "Episode finished after 1000 timesteps with reward=-22.768135082228213\n",
      "Episode finished after 500 timesteps with reward=-112.46008855185819\n",
      "Episode finished after 712 timesteps with reward=104.32063391517768\n",
      "Episode finished after 619 timesteps with reward=170.250773839831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2101/3000 [05:13<02:14,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 618 timesteps with reward=128.77128974291955\n",
      "Episode finished after 813 timesteps with reward=49.1245264580366\n",
      "iter=2100\tepsilon=0.166\n",
      "11.6153235106\n",
      "Current score(mean over 10) = 11.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2199/3000 [05:25<01:58,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 981 timesteps with reward=-562.8906134691961\n",
      "Episode finished after 1000 timesteps with reward=-64.4224717898101\n",
      "Episode finished after 1000 timesteps with reward=-73.60580026244531\n",
      "Episode finished after 1000 timesteps with reward=-87.87196222405362\n",
      "Episode finished after 1000 timesteps with reward=-82.06970777453847\n",
      "Episode finished after 1000 timesteps with reward=-76.44641999579636\n",
      "Episode finished after 1000 timesteps with reward=-85.21648434419488\n",
      "Episode finished after 1000 timesteps with reward=-73.5371041180238\n",
      "Episode finished after 1000 timesteps with reward=-38.35281494010963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2201/3000 [05:43<02:04,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-42.00268156361343\n",
      "iter=2200\tepsilon=0.155\n",
      "-118.641606048\n",
      "Current score(mean over 10) = -118.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2299/3000 [05:55<01:48,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 799 timesteps with reward=157.1867370859239\n",
      "Episode finished after 1000 timesteps with reward=-70.97647892711748\n",
      "Episode finished after 1000 timesteps with reward=-74.77194393868847\n",
      "Episode finished after 1000 timesteps with reward=-86.6801500439359\n",
      "Episode finished after 1000 timesteps with reward=-86.89797183063392\n",
      "Episode finished after 1000 timesteps with reward=-89.8991699050642\n",
      "Episode finished after 1000 timesteps with reward=-101.26441191755129\n",
      "Episode finished after 1000 timesteps with reward=-74.99281630575216\n",
      "Episode finished after 1000 timesteps with reward=-75.17163320107493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2302/3000 [06:14<01:53,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-102.89450349225852\n",
      "iter=2300\tepsilon=0.145\n",
      "-60.6362342476\n",
      "Current score(mean over 10) = -60.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 2398/3000 [06:24<01:36,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-101.26009504314693\n",
      "Episode finished after 1000 timesteps with reward=-90.31017865381892\n",
      "Episode finished after 1000 timesteps with reward=-112.7849518549688\n",
      "Episode finished after 1000 timesteps with reward=-85.18141105679341\n",
      "Episode finished after 1000 timesteps with reward=-95.5749669759585\n",
      "Episode finished after 1000 timesteps with reward=-69.77226301065357\n",
      "Episode finished after 1000 timesteps with reward=-84.726103457899\n",
      "Episode finished after 1000 timesteps with reward=-60.990430301858886\n",
      "Episode finished after 1000 timesteps with reward=-120.47687379495144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2402/3000 [06:39<01:39,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-115.08489096203802\n",
      "iter=2400\tepsilon=0.136\n",
      "-93.6162165112\n",
      "Current score(mean over 10) = -93.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2499/3000 [06:49<01:22,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-114.94847901293565\n",
      "Episode finished after 1000 timesteps with reward=-122.45368945048189\n",
      "Episode finished after 1000 timesteps with reward=-100.14788350057586\n",
      "Episode finished after 1000 timesteps with reward=-99.12105013635286\n",
      "Episode finished after 1000 timesteps with reward=-87.78474963899815\n",
      "Episode finished after 1000 timesteps with reward=-60.622672094664466\n",
      "Episode finished after 1000 timesteps with reward=-136.23602520079697\n",
      "Episode finished after 1000 timesteps with reward=-67.7160158923943\n",
      "Episode finished after 1000 timesteps with reward=-71.08858766926002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2501/3000 [07:04<01:24,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-107.89500489671187\n",
      "iter=2500\tepsilon=0.128\n",
      "-96.8014157493\n",
      "Current score(mean over 10) = -96.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 2598/3000 [07:15<01:07,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-124.3154704519046\n",
      "Episode finished after 1000 timesteps with reward=-86.96347025980538\n",
      "Episode finished after 1000 timesteps with reward=-41.11164477887141\n",
      "Episode finished after 1000 timesteps with reward=-88.07902336635551\n",
      "Episode finished after 1000 timesteps with reward=-57.96858960219164\n",
      "Episode finished after 1000 timesteps with reward=-67.6369331899701\n",
      "Episode finished after 1000 timesteps with reward=-92.3784489377565\n",
      "Episode finished after 1000 timesteps with reward=-84.7949197387238\n",
      "Episode finished after 1000 timesteps with reward=-33.53827032879925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 2602/3000 [07:30<01:08,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-105.50525823356033\n",
      "iter=2600\tepsilon=0.121\n",
      "-78.2292028888\n",
      "Current score(mean over 10) = -78.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 2699/3000 [07:43<00:51,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 337 timesteps with reward=-106.65944119269791\n",
      "Episode finished after 579 timesteps with reward=135.5778573752869\n",
      "Episode finished after 657 timesteps with reward=162.55717499753604\n",
      "Episode finished after 747 timesteps with reward=206.5613063640768\n",
      "Episode finished after 611 timesteps with reward=151.89935706050045\n",
      "Episode finished after 991 timesteps with reward=184.90582228581087\n",
      "Episode finished after 175 timesteps with reward=-79.99986116027617\n",
      "Episode finished after 616 timesteps with reward=139.29999378729525\n",
      "Episode finished after 449 timesteps with reward=125.97302284323573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 2701/3000 [07:50<00:52,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 743 timesteps with reward=141.7650890938857\n",
      "iter=2700\tepsilon=0.114\n",
      "106.188032145\n",
      "Current score(mean over 10) = 106.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2798/3000 [08:00<00:34,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps with reward=-71.54364471758369\n",
      "Episode finished after 591 timesteps with reward=117.33489411243454\n",
      "Episode finished after 584 timesteps with reward=143.53848936179298\n",
      "Episode finished after 715 timesteps with reward=143.16270133505813\n",
      "Episode finished after 731 timesteps with reward=104.51744401359717\n",
      "Episode finished after 794 timesteps with reward=119.93591669858984\n",
      "Episode finished after 542 timesteps with reward=144.64953203116562\n",
      "Episode finished after 1000 timesteps with reward=-66.46778027036903\n",
      "Episode finished after 652 timesteps with reward=104.10796316061379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2802/3000 [08:09<00:34,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 867 timesteps with reward=142.1383919628716\n",
      "iter=2800\tepsilon=0.108\n",
      "88.1373907688\n",
      "Current score(mean over 10) = 88.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2898/3000 [08:18<00:17,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 658 timesteps with reward=132.8885537396135\n",
      "Episode finished after 548 timesteps with reward=110.07429121847248\n",
      "Episode finished after 624 timesteps with reward=121.36117570180198\n",
      "Episode finished after 1000 timesteps with reward=-111.52011397212361\n",
      "Episode finished after 622 timesteps with reward=136.11528960589888\n",
      "Episode finished after 421 timesteps with reward=-113.59795592444537\n",
      "Episode finished after 722 timesteps with reward=66.47449428137827\n",
      "Episode finished after 780 timesteps with reward=177.82057879414148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2901/3000 [08:25<00:17,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 652 timesteps with reward=133.31324638973308\n",
      "Episode finished after 882 timesteps with reward=124.78773035158808\n",
      "iter=2900\tepsilon=0.102\n",
      "77.7717290186\n",
      "Current score(mean over 10) = 77.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2998/3000 [08:34<00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 482 timesteps with reward=165.39604801034244\n",
      "Episode finished after 528 timesteps with reward=-315.4655493134178\n",
      "Episode finished after 464 timesteps with reward=127.28371889912847\n",
      "Episode finished after 515 timesteps with reward=157.93665516617875\n",
      "Episode finished after 440 timesteps with reward=90.61511355096478\n",
      "Episode finished after 460 timesteps with reward=202.7148361463423\n",
      "Episode finished after 492 timesteps with reward=104.20082791289317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3000/3000 [08:39<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 563 timesteps with reward=83.93275787571962\n",
      "Episode finished after 423 timesteps with reward=140.0221036028758\n",
      "Episode finished after 567 timesteps with reward=133.62471503140293\n",
      "iter=3000\tepsilon=0.097\n",
      "89.0261226882\n",
      "Current score(mean over 10) = 89.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "lastReward = -500\n",
    "\n",
    "for i in trange(3000):    \n",
    "    \n",
    "    #play\n",
    "    for _ in range(5):\n",
    "        pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    #train\n",
    "    train_step()\n",
    "    \n",
    "    #update epsilon\n",
    "    epsilon = 0.05 + 0.95*np.exp(-epoch_counter/1000.)\n",
    "    \n",
    "    action_layer.epsilon.set_value(np.float32(epsilon))\n",
    "    \n",
    "    #play a few games for evaluation\n",
    "    if epoch_counter%100==0:\n",
    "        n_games = 10\n",
    "        lastReward = np.mean(pool.evaluate(n_games=n_games,record_video=False, verbose=False))\n",
    "        rewards[epoch_counter] = lastReward\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,action_layer.epsilon.get_value(),))\n",
    "        print(rewards[epoch_counter])\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games, lastReward))\n",
    "    \n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method evaluate in module agentnet.experiments.openai_gym.pool:\n",
      "\n",
      "evaluate(n_games=1, save_path='./records', use_monitor=True, record_video=True, verbose=True, t_max=100000) method of agentnet.experiments.openai_gym.pool.EnvPool instance\n",
      "    Plays an entire game start to end, records the logs(and possibly mp4 video), returns reward.\n",
      "    \n",
      "    :param save_path: where to save the report\n",
      "    :param record_video: if True, records mp4 video\n",
      "    :return: total reward (scalar)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pool.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: FutureWarning: pd.ewm_mean is deprecated for ndarrays and will be removed in a future version\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7a866fd860>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4P8DWlerfnNz7DMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a866e66a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import ewma\n",
    "iters,session_rewards=zip(*sorted(rewards.items(), key=lambda k_v : k_v[0]))\n",
    "plt.plot(iters,ewma(np.array(session_rewards),span=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 294 timesteps with reward=-106.87251915709271\n",
      "Episode finished after 715 timesteps with reward=158.17068437688707\n",
      "Episode finished after 649 timesteps with reward=180.5788720055919\n",
      "Episode finished after 444 timesteps with reward=147.41664351746545\n",
      "Episode finished after 1000 timesteps with reward=-44.76016677542373\n",
      "Episode finished after 489 timesteps with reward=-335.5593588567241\n",
      "Episode finished after 309 timesteps with reward=-69.7525416927676\n",
      "Episode finished after 425 timesteps with reward=115.34786240649079\n",
      "Episode finished after 418 timesteps with reward=138.25803977934652\n",
      "Episode finished after 663 timesteps with reward=116.68428589340083\n",
      "average reward: [-106.87251915709271, 158.17068437688707, 180.5788720055919, 147.41664351746545, -44.760166775423727, -335.55935885672409, -69.752541692767593, 115.34786240649079, 138.25803977934652, 116.68428589340083]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b2564375f3a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvideo_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     HTML(\"\"\"\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m<\u001b[0m\u001b[0mvideo\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"640\"\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"480\"\u001b[0m \u001b[0mcontrols\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;34m<\u001b[0m\u001b[0msource\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{}\"\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"video/mp4\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "final_reward = pool.evaluate(n_games=10,save_path=\"./records\",record_video=True)\n",
    "\n",
    "print(\"average reward:\",final_reward)\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "for video_name in video_names:\n",
    "    HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(\"./records/\"+video_name)) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
