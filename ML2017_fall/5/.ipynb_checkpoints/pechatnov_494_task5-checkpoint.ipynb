{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "# copy-paste of seminar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS='floatX=float32'\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_shape = (8,)  n_actions = 4\n"
     ]
    }
   ],
   "source": [
    "# load game and look at it\n",
    "\n",
    "import gym\n",
    "make_env = lambda: gym.make(\"LunarLander-v2\")\n",
    "\n",
    "env=make_env()\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"state_shape =\", state_shape, \" n_actions =\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEt2wCc+3igAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58ea63a7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import elu\n",
    "\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,)+state_shape)\n",
    "\n",
    "\n",
    "nn = DenseLayer(observation_layer, 60, nonlinearity=elu)\n",
    "nn = DenseLayer(nn, 60, nonlinearity=elu)\n",
    "nn = DenseLayer(nn, 40, nonlinearity=elu)\n",
    "nn = DenseLayer(nn, 20, nonlinearity=elu)\n",
    "\n",
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(nn,num_units=n_actions,\n",
    "                           nonlinearity=None,name=\"q-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking actions is done by yet another layer, that implements $ \\epsilon$ -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)\n",
    "\n",
    "#set starting epsilon\n",
    "action_layer.epsilon.set_value(np.float32(0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We define an agent entirely composed of a lasagne network:\n",
    "* Observations as InputLayer(s)\n",
    "* Actions as intermediate Layer(s)\n",
    "* `policy_estimators` is \"whatever else you want to keep track of\"\n",
    "\n",
    "Each parameter can be either one layer or a list of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              action_layers=action_layer,\n",
    "              policy_estimators=qvalues_layer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, W, b, q-values.W, q-values.b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,make_env,n_games=1,max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: [[2 2 3 2 0]]\n",
      "rewards: [[-2.36201356 -3.86702423 -1.66253905 -2.08955158  0.        ]]\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 2.44 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "obs_log,action_log,reward_log,_,_,_  = pool.interact(5)\n",
    "\n",
    "\n",
    "print('actions:',action_log)\n",
    "print('rewards:',reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll train on rollouts of 10 steps (required by n-step algorithms and rnns later)\n",
    "SEQ_LENGTH=10\n",
    "\n",
    "#load first sessions (this function calls interact and stores sessions in the pool)\n",
    "\n",
    "for _ in range(100):\n",
    "    pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "\n",
    "We shall now define a function that replays recent game sessions and updates network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2, like you implemented before in lasagne.\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,\n",
    "                                                      n_steps=1,)\n",
    "\n",
    "#compute mean loss over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get weight updates\n",
    "updates = lasagne.updates.adam(loss,weights)\n",
    "\n",
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run\n",
    "\n",
    "Play full session with an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "#untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "# from IPython.display import HTML\n",
    "# import os\n",
    "\n",
    "# video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "# HTML(\"\"\"\n",
    "# <video width=\"640\" height=\"480\" controls>\n",
    "#   <source src=\"{}\" type=\"video/mp4\">\n",
    "# </video>\n",
    "# \"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 1 #starting epoch\n",
    "rewards = {} #full game rewards\n",
    "#target_score = -90\n",
    "target_score = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=2600\tEpsilon=0.121\tCurrent score(mean over 10) = -413.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e37a27b9668e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlastReward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration=%i\\tEpsilon=%.3f\\tCurrent score(mean over %i) = %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m               \u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_games\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlastReward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastReward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtarget_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "lastReward = -500\n",
    "epoch_counter = 2600\n",
    "\n",
    "for i in trange(3000):    \n",
    "    \n",
    "    #play\n",
    "    for _ in range(5):\n",
    "        pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    #train\n",
    "    train_step()\n",
    "    \n",
    "    #update epsilon\n",
    "    epsilon = 0.05 + 0.95*np.exp(-epoch_counter/1000.)\n",
    "    \n",
    "    action_layer.epsilon.set_value(np.float32(epsilon))\n",
    "    \n",
    "    #play a few games for evaluation\n",
    "    if epoch_counter%100==0:\n",
    "        n_games = 10\n",
    "        lastReward = np.mean(pool.evaluate(n_games=n_games,record_video=False, verbose=False))\n",
    "        rewards[epoch_counter] = lastReward\n",
    "        print(\"Iteration=%i\\tEpsilon=%.3f\\tCurrent score(mean over %i) = %.3f\" % \\\n",
    "              (epoch_counter,action_layer.epsilon.get_value(),n_games, lastReward,))\n",
    "        print(\"#\" * int((lastReward + 100) // 10))\n",
    "    \n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pool.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ewma\n",
    "iters,session_rewards=zip(*sorted(rewards.items(), key=lambda k_v : k_v[0]))\n",
    "plt.plot(iters,ewma(np.array(session_rewards),span=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reward = pool.evaluate(n_games=10,save_path=\"./records\",record_video=True)\n",
    "\n",
    "print(\"average reward:\",final_reward)\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "for video_name in video_names:\n",
    "    HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(\"./records/\"+video_name)) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
