{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage actor-critic in AgentNet (5 pts)\n",
    "\n",
    "Once we're done with REINFORCE, it's time to proceed with something more sophisticated.\n",
    "The next one in line is advantage actor-critic, in which agent learns both policy and value function, using the latter to speed up learning.\n",
    "\n",
    "Your main objective for this session is to... beat MountainCar-v0... with actor-critic.\n",
    "\n",
    "Beating means making submission to [gym leaderboard](https://gym.openai.com/envs/MountainCar-v0).\n",
    "\n",
    "``` MountainCar-v0 defines \"solving\" as getting average reward of -110.0 over 100 consecutive trials. ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=gpu0\n",
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS=device=gpu0\n",
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1\n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.74741463e-03  -1.45178684e-05   1.12934511e-03  -1.59999001e-02\n",
      "   9.19512808e-02  -1.49035698e-03   8.60273570e-01   2.50423606e-03\n",
      "   1.00000000e+00   3.23557667e-02  -1.49025198e-03   8.53825212e-01\n",
      "   1.05399243e-03   1.00000000e+00   4.40814018e-01   4.45820123e-01\n",
      "   4.61422771e-01   4.89550203e-01   5.34102798e-01   6.02461040e-01\n",
      "   7.09148884e-01   8.85931849e-01   1.00000000e+00   1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v2\").env\n",
    "obs = env.reset()\n",
    "state_size = len(obs)\n",
    "\n",
    "#print(dir(env.action_space))\n",
    "#print(env.action_space.high)\n",
    "#n_actions = env.action_space.n\n",
    "actions_low = env.action_space.low\n",
    "actions_high = env.action_space.high\n",
    "action_space = env.action_space\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using shallow neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer,DenseLayer,NonlinearityLayer,batch_norm,dropout\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,state_size))\n",
    "\n",
    "nn = observation_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#a layer that predicts Qvalues\n",
    "print(action_space.shape[0])\n",
    "policy_layer = DenseLayer(nn, action_space.shape[0], nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "V_layer = DenseLayer(nn, 1, nonlinearity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.resolver.base import BaseResolver\n",
    "import theano.tensor as T\n",
    "import theano.tensor.shared_randomstreams as random_streams\n",
    "\n",
    "class ProbabilisticResolver(BaseResolver):\n",
    "\n",
    "\n",
    "    def __init__(self, incoming, assume_normalized=False, seed=1234, output_dtype='int32',\n",
    "                 name='ProbabilisticResolver'):\n",
    "        self.assume_normalized = assume_normalized\n",
    "        self.rng = random_streams.RandomStreams(seed)\n",
    "        super(ProbabilisticResolver, self).__init__(incoming, name=name,output_dtype=output_dtype)\n",
    "\n",
    "    def get_output_for(self, policy, greedy=False, **kwargs):\n",
    "        #print(dir(policy))\n",
    "        #print(policy.shape)\n",
    "        if greedy:\n",
    "            chosen_action_ids = policy\n",
    "        else:\n",
    "            batch_size, n_actions = policy.shape\n",
    "            print(batch_size, n_actions)\n",
    "            chosen_action_ids = (policy + self.rng.normal(size=policy.shape))\n",
    "            #chosen_action_ids = T.min(chosen_action_ids, T.ones_like(chosen_action_ids))\n",
    "            #chosen_action_ids = T.max(chosen_action_ids, -T.ones_like(chosen_action_ids))\n",
    "\n",
    "        return chosen_action_ids\n",
    "\n",
    "\n",
    "action_layer = ProbabilisticResolver(policy_layer,\n",
    "                                     name=\"e-greedy action picker\",\n",
    "                                     assume_normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(policy_layer,V_layer),\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params((action_layer,V_layer),trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.time_limit import TimeLimit\n",
    "\n",
    "class env_wrapper(object):\n",
    "    def __init__(self, name, t_max):\n",
    "        self.name = name\n",
    "        self.t_max = t_max\n",
    "    def __call__(self):\n",
    "        env = gym.make(self.name).env\n",
    "        env = TimeLimit(env, max_episode_steps=self.t_max)\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'agentnet.experiments.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8d98e0563f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magentnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_layer_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magentnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSessionPoolEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'agentnet.experiments.utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "A thin wrapper for openAI gym environments that maintains a set of parallel games and has a method to generate\n",
    "interaction sessions given agent one-step applier function.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from agentnet.utils.layers import get_layer_dtype\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "from warnings import warn\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "\n",
    "\n",
    "def GamePool(*args, **kwargs):\n",
    "    raise ValueError(\"Deprecated. Use EnvPool(agent,env_title,n_parallel_agents) instead\")\n",
    "\n",
    "\n",
    "deprecated_preprocess_obs = lambda obs: obs\n",
    "\n",
    "# A whole lot of space invaders\n",
    "class EnvPool(object):\n",
    "    def __init__(self, agent, make_env=lambda: gym.make(\"SpaceInvaders-v0\"), n_games=1, max_size=None,\n",
    "                 preprocess_observation=deprecated_preprocess_obs, agent_step=None):\n",
    "        \"\"\"A pool that stores several\n",
    "           - game states (gym environment)\n",
    "           - prev observations - last agent observations\n",
    "           - prev memory states - last agent hidden states\n",
    "        and is capable of some auxilary actions like evaluating agent on one game session (See .evaluate()).\n",
    "        :param agent: Agent which interacts with the environment.\n",
    "        :type agent: agent.Agent\n",
    "        :param make_env: Factory that produces environments OR a name of the gym environment.\n",
    "                See gym.envs.registry.all()\n",
    "        :type make_env: function or str\n",
    "        :param n_games: Number of parallel games. One game by default.\n",
    "        :type n_games: int\n",
    "        :param max_size: Max pool size by default (if appending sessions). By default, pool is not constrained in size.\n",
    "        :type max_size: int\n",
    "        :param preprocess_observation: Function for preprocessing raw observations from gym env to agent format.\n",
    "            By default it is identity function.\n",
    "        :type preprocess_observation: function\n",
    "        :param agent_step: Function with the same signature as agent.get_react_function().\n",
    "        :type agent_step: theano.function\n",
    "        \"\"\"\n",
    "        if not callable(make_env):\n",
    "            env_name = make_env\n",
    "            make_env = lambda: gym.make(env_name)\n",
    "\n",
    "        ##Deprecation warning\n",
    "        if preprocess_observation != deprecated_preprocess_obs:\n",
    "            warn(\"preprocess_observation is deprecated (will be removed in 0.11). Use gym.core.Wrapper instead.\")\n",
    "\n",
    "        # Create atari games.\n",
    "        self.make_env = make_env\n",
    "        self.envs = [self.make_env() for _ in range(n_games)]\n",
    "        self.preprocess_observation = preprocess_observation\n",
    "\n",
    "        # Initial observations.\n",
    "        self.prev_observations = [self.preprocess_observation(make_env.reset()) for make_env in self.envs]\n",
    "\n",
    "        # Agent memory variables (if you use recurrent networks).\n",
    "        self.prev_memory_states = [np.zeros((n_games,) + tuple(mem.output_shape[1:]),\n",
    "                                            dtype=get_layer_dtype(mem))\n",
    "                                   for mem in agent.agent_states]\n",
    "\n",
    "        # Save agent.\n",
    "        self.agent = agent\n",
    "        self.agent_step = agent_step or agent.get_react_function()\n",
    "\n",
    "        # Create experience replay environment.\n",
    "        self.experience_replay = SessionPoolEnvironment(observations=agent.observation_layers,\n",
    "                                                        actions=agent.action_layers,\n",
    "                                                        agent_memories=agent.agent_states)\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # Whether particular session has just been terminated and needs restarting.\n",
    "        self.just_ended = [False] * len(self.envs)\n",
    "\n",
    "    def interact(self, n_steps=100, verbose=False, add_last_observation=True):\n",
    "        \"\"\"Generate interaction sessions with ataries (openAI gym atari environments)\n",
    "        Sessions will have length n_steps. Each time one of games is finished, it is immediately getting reset\n",
    "        and this time is recorded in is_alive_log (See returned values).\n",
    "        :param n_steps: Length of an interaction.\n",
    "        :param verbose: If True, prints small debug message whenever a game gets reloaded after end.\n",
    "        :param add_last_observation: If True, appends the final state with\n",
    "                state=final_state,\n",
    "                action=-1,\n",
    "                reward=0,\n",
    "                new_memory_states=prev_memory_states, effectively making n_steps-1 records.\n",
    "        :returns: observation_log, action_log, reward_log, [memory_logs], is_alive_log, info_log\n",
    "        :rtype: a bunch of tensors [batch, tick, size...],\n",
    "                the only exception is info_log, which is a list of infos for [time][batch], None padded tick\n",
    "        \"\"\"\n",
    "\n",
    "        def env_step(i, action):\n",
    "            \"\"\"Environment reaction.\n",
    "            :returns: observation, reward, is_alive, info\n",
    "            \"\"\"\n",
    "\n",
    "            if not self.just_ended[i]:\n",
    "                new_observation, cur_reward, is_done, info = self.envs[i].step(action)\n",
    "                if is_done:\n",
    "                    # Game ends now, will finalize on next tick.\n",
    "                    self.just_ended[i] = True\n",
    "                new_observation = self.preprocess_observation(new_observation)\n",
    "\n",
    "                # note: is_alive=True in any case because environment is still alive (last tick alive) in our notation.\n",
    "                return new_observation, cur_reward, True, info\n",
    "            else:\n",
    "                # Reset environment, get new observation to be used on next tick.\n",
    "                new_observation = self.preprocess_observation(self.envs[i].reset())\n",
    "\n",
    "                # Reset memory for new episode.\n",
    "                for m_i in range(len(new_memory_states)):\n",
    "                    new_memory_states[m_i][i] = 0\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"env %i reloaded\" % i)\n",
    "\n",
    "                self.just_ended[i] = False\n",
    "\n",
    "                return new_observation, 0, False, {'end': True}\n",
    "\n",
    "        history_log = []\n",
    "\n",
    "        for i in range(n_steps - int(add_last_observation)):\n",
    "            res = self.agent_step(self.prev_observations, *self.prev_memory_states)\n",
    "            actions, new_memory_states = res[0], res[1:]\n",
    "\n",
    "            new_observations, cur_rewards, is_alive, infos = zip(*map(env_step, range(len(self.envs)), actions))\n",
    "\n",
    "            # Append data tuple for this tick.\n",
    "            history_log.append((self.prev_observations, actions, cur_rewards, new_memory_states, is_alive, infos))\n",
    "\n",
    "            self.prev_observations = new_observations\n",
    "            self.prev_memory_states = new_memory_states\n",
    "\n",
    "        if add_last_observation:\n",
    "            fake_actions = np.array([env.action_space.sample() for env in self.envs])\n",
    "            fake_rewards = np.zeros(shape=len(self.envs))\n",
    "            fake_is_alive = np.ones(shape=len(self.envs))\n",
    "            history_log.append((self.prev_observations, fake_actions, fake_rewards, self.prev_memory_states,\n",
    "                                fake_is_alive, [None] * len(self.envs)))\n",
    "\n",
    "        # cast to numpy arrays\n",
    "        observation_log, action_log, reward_log, memories_log, is_alive_log, info_log = zip(*history_log)\n",
    "\n",
    "        # tensor dimensions\n",
    "        # [batch_i, time_i, observation_size...]\n",
    "        observation_log = np.array(observation_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch, time, units] for each memory tensor\n",
    "        memories_log = list(map(lambda mem: np.array(mem).swapaxes(0, 1), zip(*memories_log)))\n",
    "\n",
    "        # [batch_i,time_i]\n",
    "        action_log = np.array(action_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch_i, time_i]\n",
    "        reward_log = np.array(reward_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch_i, time_i]\n",
    "        is_alive_log = np.array(is_alive_log).swapaxes(0, 1).astype('uint8')\n",
    "\n",
    "        return observation_log, action_log, reward_log, memories_log, is_alive_log, info_log\n",
    "\n",
    "    def update(self, n_steps=100, append=False, max_size=None, add_last_observation=True,\n",
    "               preprocess=lambda observations, actions, rewards, is_alive, h0: (\n",
    "                       observations, actions, rewards, is_alive, h0)):\n",
    "        \"\"\"Create new sessions and add them into the pool.\n",
    "        :param n_steps: How many time steps in each session.\n",
    "        :param append: If True, appends sessions to the pool and crops at max_size.\n",
    "            Otherwise, old sessions will be thrown away entirely.\n",
    "        :param max_size: If not None, substitutes default max_size (from __init__) for this update only.\n",
    "        :param add_last_observation: See param `add_last_observation` in `.interact()` method.\n",
    "        :param preprocess: Function that implements arbitrary processing of the sessions.\n",
    "            Takes AND outputs (observation_tensor, action_tensor, reward_tensor, is_alive_tensor, preceding_memory_states).\n",
    "            For param specs see `.interact()` output format.\n",
    "        \"\"\"\n",
    "\n",
    "        preceding_memory_states = list(self.prev_memory_states)\n",
    "\n",
    "        # Get interaction sessions.\n",
    "        observation_tensor, action_tensor, reward_tensor, _, is_alive_tensor, _ = self.interact(n_steps=n_steps,\n",
    "                                                                                                add_last_observation=add_last_observation)\n",
    "\n",
    "        observation_tensor, action_tensor, reward_tensor, is_alive_tensor, preceding_memory_states = \\\n",
    "            preprocess(observation_tensor, action_tensor, reward_tensor, is_alive_tensor, preceding_memory_states)\n",
    "\n",
    "        # Load them into experience replay environment.\n",
    "        if not append:\n",
    "            self.experience_replay.load_sessions(observation_tensor, action_tensor, reward_tensor,\n",
    "                                                 is_alive_tensor, preceding_memory_states)\n",
    "        else:\n",
    "            self.experience_replay.append_sessions(observation_tensor, action_tensor, reward_tensor,\n",
    "                                                   is_alive_tensor, preceding_memory_states,\n",
    "                                                   max_pool_size=max_size or self.max_size)\n",
    "\n",
    "    def evaluate(self, n_games=1, save_path=\"./records\", use_monitor=True, record_video=True, verbose=True,\n",
    "                 t_max=100000):\n",
    "        \"\"\"Plays an entire game start to end, records the logs(and possibly mp4 video), returns reward.\n",
    "        :param save_path: where to save the report\n",
    "        :param record_video: if True, records mp4 video\n",
    "        :return: total reward (scalar)\n",
    "        \"\"\"\n",
    "        env = self.make_env()\n",
    "\n",
    "        if not use_monitor and record_video:\n",
    "            raise warn(\"Cannot video without gym monitor. If you still want video, set use_monitor to True\")\n",
    "\n",
    "        if record_video :\n",
    "            env = Monitor(env,save_path,force=True)\n",
    "        elif use_monitor:\n",
    "            env = Monitor(env, save_path, video_callable=lambda i: False, force=True)\n",
    "\n",
    "        game_rewards = []\n",
    "        for _ in range(n_games):\n",
    "            # initial observation\n",
    "            observation = env.reset()\n",
    "            # initial memory\n",
    "            prev_memories = [np.zeros((1,) + tuple(mem.output_shape[1:]),\n",
    "                                      dtype=get_layer_dtype(mem))\n",
    "                             for mem in self.agent.agent_states]\n",
    "\n",
    "            t = 0\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "\n",
    "                res = self.agent_step(self.preprocess_observation(observation)[None, ...], *prev_memories)\n",
    "                action, new_memories = res[0], res[1:]\n",
    "\n",
    "                observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "                total_reward += reward\n",
    "                prev_memories = new_memories\n",
    "\n",
    "                if done or t >= t_max:\n",
    "                    if verbose:\n",
    "                        print(\"Episode finished after {} timesteps with reward={}\".format(t + 1, total_reward))\n",
    "                    break\n",
    "                t += 1\n",
    "            game_rewards.append(total_reward)\n",
    "\n",
    "        env.close()\n",
    "        del env\n",
    "        return game_rewards\n",
    "\n",
    "#create a small pool with 10 parallel agents\n",
    "pool = EnvPool(agent,make_env=env_wrapper(\"BipedalWalker-v2\", 10000), n_games=24,max_size=1000) \n",
    "\n",
    "#we assume that pool size 1000 is small enough to learn \"almost on policy\" :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 ms, sys: 8 ms, total: 60 ms\n",
      "Wall time: 60.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "#print(action_log[:3])\n",
    "#print(reward_log[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Wrong number of dimensions: expected 2, got 3 with shape (24, 100, 4).', 'Container name \"session.actions_history.0\"')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-86e45f9480c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#load first sessions (this function calls interact and remembers sessions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/agentnet-0.10.6-py3.5.egg/agentnet/experiments/openai_gym/pool.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n_steps, append, max_size, add_last_observation, preprocess)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             self.experience_replay.load_sessions(observation_tensor, action_tensor, reward_tensor,\n\u001b[0;32m--> 202\u001b[0;31m                                                  is_alive_tensor, preceding_memory_states)\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             self.experience_replay.append_sessions(observation_tensor, action_tensor, reward_tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/agentnet-0.10.6-py3.5.egg/agentnet/environment/session_pool.py\u001b[0m in \u001b[0;36mload_sessions\u001b[0;34m(self, observation_sequences, action_sequences, reward_seq, is_alive, prev_memories)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mset_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mset_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mset_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/agentnet-0.10.6-py3.5.egg/agentnet/utils/shared.py\u001b[0m in \u001b[0;36mset_shared\u001b[0;34m(var, value)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mval_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gpuarray fix by @kashif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/sharedvalue.py\u001b[0m in \u001b[0;36mset_value\u001b[0;34m(self, new_value, borrow)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/link.py\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    477\u001b[0m                                                            **kwargs)\n\u001b[1;32m    478\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/tensor/type.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    176\u001b[0m             raise TypeError(\"Wrong number of dimensions: expected %s,\"\n\u001b[1;32m    177\u001b[0m                             \" got %s with shape %s.\" % (self.ndim, data.ndim,\n\u001b[0;32m--> 178\u001b[0;31m                                                         data.shape))\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Wrong number of dimensions: expected 2, got 3 with shape (24, 100, 4).', 'Container name \"session.actions_history.0\"')"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 100\n",
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic loss\n",
    "\n",
    "Here we define obective function for actor-critic (one-step) RL.\n",
    "\n",
    "* We regularize policy with expected inverse action probabilities (discouraging very small probas) to make objective numerically stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "\n",
    "_,_,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.learning import a2c                                                   \n",
    "\n",
    "\n",
    "elwise_mse_loss = a2c.get_elementwise_objective(policy_seq,\n",
    "                                                V_seq[:,:,0],\n",
    "                                                replay.actions[0],\n",
    "                                                replay.rewards,\n",
    "                                                replay.is_alive,\n",
    "                                                gamma_or_gammas=0.99,\n",
    "                                                n_steps=1)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()\n",
    "\n",
    "loss += 0.0001*(1./(policy_seq)).sum(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import tensor as T\n",
    "# loss += <regularize agent with negative entropy. Higher entropy = smaller loss. Multiply by small coefficient>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.rmsprop(loss, weights, learning_rate=10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)\n",
    "\n",
    "#video is in the ./records folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "loss = 0\n",
    "for i in tqdm.tnrange(100000):    \n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    loss = loss*0.99 + train_step()*0.01\n",
    "        \n",
    "    \n",
    "    \n",
    "    if epoch_counter%100==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = np.average(pool.experience_replay.rewards.get_value()[:,:-1],\n",
    "                                      weights=1+pool.experience_replay.is_alive.get_value()[:,:-1])\n",
    "        print(\"iter=%i\\treward/step=%.5f\\tloss ma=%.5f\"%(epoch_counter,\n",
    "                                                        pool_mean_reward,\n",
    "                                                        loss))\n",
    "        \n",
    "\n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%500 ==0:\n",
    "        clear_output(True)\n",
    "\n",
    "        n_games = 10\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,\n",
    "                                               verbose=False)\n",
    "        iters,session_rewards=zip(*sorted(rewards.items(), key=lambda x: x[0])) #key=lambda (k,v):k))\n",
    "        mean_rewards = [np.mean(x) for x in session_rewards]\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(iters, mean_rewards)\n",
    "        plt.show()\n",
    "        if np.mean(rewards[epoch_counter]) > -105:\n",
    "            break\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "        \n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda (k,v):k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iters,map(np.mean,session_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = EnvPool(agent,make_env=\"MountainCar-v0\", \n",
    "               n_games=1,\n",
    "               max_size=1000) \n",
    "pool.evaluate(record_video=False,\n",
    "              n_games=10000, \n",
    "              verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym.upload('/home/common/nexes/Practical_RL/week6/records',\n",
    "#            api_key=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "e01984c5a4884741b744c8723712c5bb": {
     "views": [
      {
       "cell_index": 30
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
