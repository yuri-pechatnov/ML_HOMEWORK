{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[this demo requires doom installed either from gym-pool or from [ppaquette's repo](https://github.com/ppaquette/gym-doom)]\n",
    "\n",
    "## Basic Doom demo\n",
    "\n",
    "This demo solves DoomBasic env with a simple q-learning with experience replay.\n",
    "\n",
    "Video observation forces you to use ```CNN```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nexes/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=gpu,floatX=float32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%env THEANO_FLAGS=device=gpu,floatX=float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "class Wrapper(ObservationWrapper):\n",
    "    \n",
    "    def _observation(self,img):    \n",
    "        a = img[0][23:-30, 10:-8]\n",
    "        v_min, v_max = np.percentile(a, (1.2, 98.8))\n",
    "        better_contrast = skimage.exposure.rescale_intensity(a, in_range=(v_min, v_max))\n",
    "        return np.expand_dims(a, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:01:49,013] Making new env: ppaquette/DoomBasic-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from gym.wrappers import SkipWrapper\n",
    "from ppaquette_gym_doom.wrappers.action_space import ToDiscrete\n",
    "from agentnet.experiments.openai_gym.wrappers import PreprocessImage\n",
    "GAME_NAME = 'ppaquette/DoomBasic-v0'\n",
    "\n",
    "make_env = lambda: Wrapper(PreprocessImage(SkipWrapper(4)(ToDiscrete(\"minimal\")(gym.make(GAME_NAME))),\n",
    "                                   width=80,height=80,grayscale=True))\n",
    "\n",
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 27, 62)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global params.\n",
    "observation_shape = env.reset().shape\n",
    "n_actions = env.action_space.n\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "SEQ_LENGTH = 10\n",
    "FRAME_NUMBER = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = obs[0][20:-20, 10:-8]\n",
    "# v_min, v_max = np.percentile(a, (1.5, 99))\n",
    "# better_contrast = skimage.exposure.rescale_intensity(a, in_range=(v_min, v_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 27, 62)\n",
      "-10.0 False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10d9d1950>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAACwCAYAAADE1j9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105dbe790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "obs,r,done,_=env.step(1)\n",
    "print obs.shape\n",
    "# a = obs[20:-20, 10:-8]\n",
    "# print a.shape\n",
    "# v_min, v_max = np.percentile(a, (1.2, 98.8))\n",
    "# better_contrast = skimage.exposure.rescale_intensity(a, in_range=(v_min, v_max))\n",
    "print r, done\n",
    "plt.imshow(obs[0],cmap='gray',interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n",
    "![scheme](https://s18.postimg.org/gbmsq6gmx/dqn_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=gpu0,floatX=float32\n"
     ]
    }
   ],
   "source": [
    "#setup and import theano/lasagne. Prefer GPU\n",
    "%env THEANO_FLAGS=device=gpu0,floatX=float32\n",
    "\n",
    "import theano, lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "from agentnet.memory import WindowAugmentation, LSTMCell\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.target_network import TargetNetwork\n",
    "from agentnet.experiments.openai_gym.pool import EnvPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation\n",
    "observation_layer = InputLayer((None,)+observation_shape,)\n",
    "\n",
    "#4-tick window over images\n",
    "\n",
    "prev_wnd = InputLayer((None,FRAME_NUMBER)+observation_shape) \n",
    "new_wnd = WindowAugmentation(observation_layer,prev_wnd)\n",
    "        \n",
    "#reshape to (frame, h,w). If you don't use grayscale, 4 should become 12.\n",
    "wnd_reshape = reshape(new_wnd, (-1,FRAME_NUMBER*observation_shape[0])+observation_shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define your NN which probably solve the environment. \n",
    "\n",
    "#### Tips:\n",
    "1. Main component are likely to be ```Conv2D``` and ```Pool2DLayer```\n",
    "2. Batch normalization here might speeds up training but may get unstable if you use small experience replay buffer\n",
    "3. Last layers should be Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 4, 1, 27, 62)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_wnd.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = lasagne.layers.Conv2DLayer(wnd_reshape, 32, 5,  stride = 3)\n",
    "batch_norm = lasagne.layers.batch_norm(nn)\n",
    "nn = lasagne.layers.Conv2DLayer(batch_norm, 64, 3, stride=2)\n",
    "batch_norm = lasagne.layers.batch_norm(nn)\n",
    "dense = lasagne.layers.DenseLayer(nn, 256, nonlinearity=lasagne.nonlinearities.elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qvalues layer\n",
    "qvalues_layer = DenseLayer(dense, n_actions, nonlinearity=None, name='qval')\n",
    "\n",
    "#assert qvalues_layer.nonlinearity is not rectify\n",
    "#baseline for all qvalues\n",
    "#qvalues_layer = DenseLayer(<fill params>, name='qval')\n",
    "        \n",
    "#sample actions proportionally to policy_layer\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetnet = TargetNetwork(qvalues_layer)\n",
    "qvalues_old = targetnet.output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(qvalues_layer,qvalues_old),\n",
    "              agent_states={new_wnd:prev_wnd},\n",
    "              action_layers=action_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, beta, gamma, W, W, b, qval.W, qval.b]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:04:50,240] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:04:50,267] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:04:50,289] Making new env: ppaquette/DoomBasic-v0\n"
     ]
    }
   ],
   "source": [
    "pool = EnvPool(agent,make_env, \n",
    "               n_games=3, #parallel games (only 1 so far)\n",
    "               max_size=20000) #experience replay pool holding last 1k sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 2 2 2 2 1]\n",
      " [0 2 2 2 2 2 0]\n",
      " [0 2 2 2 2 2 3]]\n",
      "[[-5. -5. -5. -5. -5. -5.  0.]\n",
      " [-5. -5. -5. -5. -5. -5.  0.]\n",
      " [-5. -5. -5. -5. -5. -5.  0.]]\n",
      "CPU times: user 176 ms, sys: 16.2 ms, total: 192 ms\n",
      "Wall time: 667 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "print(action_log)\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100,replace=True)\n",
    "\n",
    "_,_,_,_,(qvalues_seq,old_qvalues_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      qvalues_target=old_qvalues_seq,\n",
    "                                                      gamma_or_gammas=0.99)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.adam(loss, weights, learning_rate= 1e-4)\n",
    "\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:07:32,793] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:07:32,853] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-03-19 22:07:34,259] Starting new video recorder writing to /Users/nexes/Documents/Study/Practical_RL/week4/records_doom2/openaigym.video.0.41826.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-430.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:07:37,116] Starting new video recorder writing to /Users/nexes/Documents/Study/Practical_RL/week4/records_doom2/openaigym.video.0.41826.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-360.0\n",
      "Episode finished after 70 timesteps with reward=-430.0\n",
      "Episode finished after 8 timesteps with reward=61.0\n",
      "Episode finished after 70 timesteps with reward=-430.0\n",
      "Episode finished after 70 timesteps with reward=-435.0\n",
      "Episode finished after 70 timesteps with reward=-435.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:07:48,423] Starting new video recorder writing to /Users/nexes/Documents/Study/Practical_RL/week4/records_doom2/openaigym.video.0.41826.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-360.0\n",
      "Episode finished after 70 timesteps with reward=-435.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:07:52,904] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records_doom2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-410.0\n"
     ]
    }
   ],
   "source": [
    "action_layer.epsilon.set_value(0)\n",
    "untrained_reward = np.mean(pool.evaluate(save_path=\"./records_doom2\",record_video=True,n_games=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #show video\n",
    "# from IPython.display import HTML\n",
    "# import os\n",
    "\n",
    "# video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records_doom2/\")))\n",
    "\n",
    "# HTML(\"\"\"\n",
    "# <video width=\"640\" height=\"480\" controls>\n",
    "#   <source src=\"{}\" type=\"video/mp4\">\n",
    "# </video>\n",
    "# \"\"\".format(\"./records_doom2/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {0:untrained_reward}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have ```tqdm```, remove the first line and ```tqdm_notebook``` from second line\n",
    "\n",
    "Loop may take years to finish.\n",
    "\n",
    "You may consider interrupting early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=10\tepsilon=0.954\treward/step=-5.24242\n",
      "iter=20\tepsilon=0.910\treward/step=-4.50952\n",
      "iter=30\tepsilon=0.868\treward/step=-4.40645\n",
      "iter=40\tepsilon=0.828\treward/step=-4.47561\n",
      "iter=50\tepsilon=0.790\treward/step=-4.17451\n",
      "iter=60\tepsilon=0.754\treward/step=-4.27541\n",
      "iter=70\tepsilon=0.719\treward/step=-4.35399\n",
      "iter=80\tepsilon=0.687\treward/step=-4.17737\n",
      "iter=90\tepsilon=0.656\treward/step=-3.91722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:11:54,962] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:11:54,992] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=100\tepsilon=0.626\treward/step=-3.68053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:11:56,579] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 12 timesteps with reward=29.0\n",
      "iter=110\tepsilon=0.598\treward/step=-3.66006\n",
      "iter=120\tepsilon=0.571\treward/step=-3.66556\n",
      "iter=130\tepsilon=0.546\treward/step=-3.76209\n",
      "iter=140\tepsilon=0.522\treward/step=-3.68274\n",
      "iter=150\tepsilon=0.499\treward/step=-3.76137\n",
      "iter=160\tepsilon=0.477\treward/step=-3.75859\n",
      "iter=170\tepsilon=0.456\treward/step=-3.70838\n",
      "iter=180\tepsilon=0.436\treward/step=-3.64788\n",
      "iter=190\tepsilon=0.417\treward/step=-3.58883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:15:54,770] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:15:54,816] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=200\tepsilon=0.399\treward/step=-3.59934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:15:58,505] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 63 timesteps with reward=-288.0\n",
      "iter=210\tepsilon=0.382\treward/step=-3.65087\n",
      "iter=220\tepsilon=0.366\treward/step=-3.66274\n",
      "iter=230\tepsilon=0.351\treward/step=-3.64069\n",
      "iter=240\tepsilon=0.336\treward/step=-3.61715\n",
      "iter=250\tepsilon=0.322\treward/step=-3.60651\n",
      "iter=260\tepsilon=0.309\treward/step=-3.59080\n",
      "iter=270\tepsilon=0.296\treward/step=-3.47331\n",
      "iter=280\tepsilon=0.284\treward/step=-3.44626\n",
      "iter=290\tepsilon=0.273\treward/step=-3.34101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:19:39,188] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:19:39,210] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=300\tepsilon=0.262\treward/step=-3.34319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:19:40,578] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 8 timesteps with reward=61.0\n",
      "iter=310\tepsilon=0.252\treward/step=-3.26238\n",
      "iter=320\tepsilon=0.242\treward/step=-3.21308\n",
      "iter=330\tepsilon=0.232\treward/step=-3.18590\n",
      "iter=340\tepsilon=0.224\treward/step=-3.12346\n",
      "iter=350\tepsilon=0.215\treward/step=-3.14179\n",
      "iter=360\tepsilon=0.207\treward/step=-3.06962\n",
      "iter=370\tepsilon=0.199\treward/step=-2.98868\n",
      "iter=380\tepsilon=0.192\treward/step=-2.99151\n",
      "iter=390\tepsilon=0.185\treward/step=-3.00384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:23:28,855] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:23:28,905] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=400\tepsilon=0.179\treward/step=-2.97298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:23:30,364] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1 timesteps with reward=96.0\n",
      "iter=410\tepsilon=0.172\treward/step=-2.89011\n",
      "iter=420\tepsilon=0.166\treward/step=-2.78931\n",
      "iter=430\tepsilon=0.161\treward/step=-2.79745\n",
      "iter=440\tepsilon=0.155\treward/step=-2.77173\n",
      "iter=450\tepsilon=0.150\treward/step=-2.75055\n",
      "iter=460\tepsilon=0.145\treward/step=-2.69798\n",
      "iter=470\tepsilon=0.141\treward/step=-2.63156\n",
      "iter=480\tepsilon=0.136\treward/step=-2.61538\n",
      "iter=490\tepsilon=0.132\treward/step=-2.62974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:27:35,531] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:27:35,558] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=500\tepsilon=0.128\treward/step=-2.61118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:27:38,610] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-360.0\n",
      "iter=510\tepsilon=0.124\treward/step=-2.60391\n",
      "iter=520\tepsilon=0.121\treward/step=-2.61446\n",
      "iter=530\tepsilon=0.117\treward/step=-2.58782\n",
      "iter=540\tepsilon=0.114\treward/step=-2.57400\n",
      "iter=550\tepsilon=0.111\treward/step=-2.56612\n",
      "iter=560\tepsilon=0.108\treward/step=-2.53440\n",
      "iter=570\tepsilon=0.105\treward/step=-2.55931\n",
      "iter=580\tepsilon=0.102\treward/step=-2.54073\n",
      "iter=590\tepsilon=0.100\treward/step=-2.55251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:32:03,540] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:32:03,562] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=600\tepsilon=0.097\treward/step=-2.51681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:32:04,922] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 5 timesteps with reward=76.0\n",
      "iter=610\tepsilon=0.095\treward/step=-2.48783\n",
      "iter=620\tepsilon=0.093\treward/step=-2.49436\n",
      "iter=630\tepsilon=0.091\treward/step=-2.49435\n",
      "iter=640\tepsilon=0.089\treward/step=-2.47733\n",
      "iter=650\tepsilon=0.087\treward/step=-2.40896\n",
      "iter=660\tepsilon=0.085\treward/step=-2.41155\n",
      "iter=670\tepsilon=0.083\treward/step=-2.36468\n",
      "iter=680\tepsilon=0.082\treward/step=-2.30382\n",
      "iter=690\tepsilon=0.080\treward/step=-2.25075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:36:31,931] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:36:31,963] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=700\tepsilon=0.079\treward/step=-2.23471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:36:33,270] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 3 timesteps with reward=86.0\n",
      "iter=710\tepsilon=0.077\treward/step=-2.21496\n",
      "iter=720\tepsilon=0.076\treward/step=-2.13130\n",
      "iter=730\tepsilon=0.075\treward/step=-2.09188\n",
      "iter=740\tepsilon=0.073\treward/step=-2.00180\n",
      "iter=750\tepsilon=0.072\treward/step=-1.96622\n",
      "iter=760\tepsilon=0.071\treward/step=-1.85642\n",
      "iter=770\tepsilon=0.070\treward/step=-1.76952\n",
      "iter=780\tepsilon=0.069\treward/step=-1.71131\n",
      "iter=790\tepsilon=0.068\treward/step=-1.64648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:41:05,965] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:41:05,987] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=800\tepsilon=0.067\treward/step=-1.57486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:41:07,328] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps with reward=71.0\n",
      "iter=810\tepsilon=0.067\treward/step=-1.49092\n",
      "iter=820\tepsilon=0.066\treward/step=-1.38352\n",
      "iter=830\tepsilon=0.065\treward/step=-1.27020\n",
      "iter=840\tepsilon=0.064\treward/step=-1.20575\n",
      "iter=850\tepsilon=0.064\treward/step=-1.12538\n",
      "iter=860\tepsilon=0.063\treward/step=-1.03891\n",
      "iter=870\tepsilon=0.062\treward/step=-0.97134\n",
      "iter=880\tepsilon=0.062\treward/step=-0.90193\n",
      "iter=890\tepsilon=0.061\treward/step=-0.82840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:46:01,770] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:46:01,800] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=900\tepsilon=0.061\treward/step=-0.76937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:46:03,374] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 5 timesteps with reward=76.0\n",
      "iter=910\tepsilon=0.060\treward/step=-0.73864\n",
      "iter=920\tepsilon=0.060\treward/step=-0.68882\n",
      "iter=930\tepsilon=0.059\treward/step=-0.62470\n",
      "iter=940\tepsilon=0.059\treward/step=-0.60312\n",
      "iter=950\tepsilon=0.058\treward/step=-0.51595\n",
      "iter=960\tepsilon=0.058\treward/step=-0.47385\n",
      "iter=970\tepsilon=0.057\treward/step=-0.42911\n",
      "iter=980\tepsilon=0.057\treward/step=-0.35688\n",
      "iter=990\tepsilon=0.057\treward/step=-0.34820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:51:27,486] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:51:27,518] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1000\tepsilon=0.056\treward/step=-0.28898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:51:28,877] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps with reward=71.0\n",
      "iter=1010\tepsilon=0.056\treward/step=-0.18764\n",
      "iter=1020\tepsilon=0.056\treward/step=-0.07835\n",
      "iter=1030\tepsilon=0.056\treward/step=0.01814\n",
      "iter=1040\tepsilon=0.055\treward/step=0.08127\n",
      "iter=1050\tepsilon=0.055\treward/step=0.15801\n",
      "iter=1060\tepsilon=0.055\treward/step=0.21894\n",
      "iter=1070\tepsilon=0.055\treward/step=0.29763\n",
      "iter=1080\tepsilon=0.054\treward/step=0.38042\n",
      "iter=1090\tepsilon=0.054\treward/step=0.46493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:56:44,517] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 22:56:44,540] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1100\tepsilon=0.054\treward/step=0.53473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 22:56:45,898] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 7 timesteps with reward=66.0\n",
      "iter=1110\tepsilon=0.054\treward/step=0.56121\n",
      "iter=1120\tepsilon=0.054\treward/step=0.64199\n",
      "iter=1130\tepsilon=0.053\treward/step=0.73015\n",
      "iter=1140\tepsilon=0.053\treward/step=0.82004\n",
      "iter=1150\tepsilon=0.053\treward/step=0.91798\n",
      "iter=1160\tepsilon=0.053\treward/step=1.00801\n",
      "iter=1170\tepsilon=0.053\treward/step=1.10239\n",
      "iter=1180\tepsilon=0.053\treward/step=1.20155\n",
      "iter=1190\tepsilon=0.052\treward/step=1.28931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:02:11,194] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 23:02:11,215] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1200\tepsilon=0.052\treward/step=1.37044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:02:12,563] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps with reward=71.0\n",
      "iter=1210\tepsilon=0.052\treward/step=1.46141\n",
      "iter=1220\tepsilon=0.052\treward/step=1.54518\n",
      "iter=1230\tepsilon=0.052\treward/step=1.61051\n",
      "iter=1240\tepsilon=0.052\treward/step=1.68555\n",
      "iter=1250\tepsilon=0.052\treward/step=1.73888\n",
      "iter=1260\tepsilon=0.052\treward/step=1.82657\n",
      "iter=1270\tepsilon=0.052\treward/step=1.90711\n",
      "iter=1280\tepsilon=0.052\treward/step=1.99477\n",
      "iter=1290\tepsilon=0.052\treward/step=2.07245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:08:07,905] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 23:08:07,937] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1300\tepsilon=0.051\treward/step=2.17107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:08:09,351] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps with reward=71.0\n",
      "iter=1310\tepsilon=0.051\treward/step=2.24045\n",
      "iter=1320\tepsilon=0.051\treward/step=2.32332\n",
      "iter=1330\tepsilon=0.051\treward/step=2.40433\n",
      "iter=1340\tepsilon=0.051\treward/step=2.49224\n",
      "iter=1350\tepsilon=0.051\treward/step=2.54949\n",
      "iter=1360\tepsilon=0.051\treward/step=2.61193\n",
      "iter=1370\tepsilon=0.051\treward/step=2.67328\n",
      "iter=1380\tepsilon=0.051\treward/step=2.74873\n",
      "iter=1390\tepsilon=0.051\treward/step=2.82075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:14:12,079] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 23:14:12,109] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1400\tepsilon=0.051\treward/step=2.89405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:14:13,532] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps with reward=71.0\n",
      "iter=1410\tepsilon=0.051\treward/step=2.96426\n",
      "iter=1420\tepsilon=0.051\treward/step=3.02587\n",
      "iter=1430\tepsilon=0.051\treward/step=3.09658\n",
      "iter=1440\tepsilon=0.051\treward/step=3.16380\n",
      "iter=1450\tepsilon=0.051\treward/step=3.22977\n",
      "iter=1460\tepsilon=0.051\treward/step=3.29747\n",
      "iter=1470\tepsilon=0.051\treward/step=3.36125\n",
      "iter=1480\tepsilon=0.051\treward/step=3.43212\n",
      "iter=1490\tepsilon=0.051\treward/step=3.49729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:20:55,535] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 23:20:55,585] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1500\tepsilon=0.051\treward/step=3.55887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:20:56,954] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 4 timesteps with reward=81.0\n",
      "iter=1510\tepsilon=0.050\treward/step=3.63170\n",
      "iter=1520\tepsilon=0.050\treward/step=3.68008\n",
      "iter=1530\tepsilon=0.050\treward/step=3.73229\n",
      "iter=1540\tepsilon=0.050\treward/step=3.80039\n",
      "iter=1550\tepsilon=0.050\treward/step=3.85594\n",
      "iter=1560\tepsilon=0.050\treward/step=3.91296\n",
      "iter=1570\tepsilon=0.050\treward/step=3.96522\n",
      "iter=1580\tepsilon=0.050\treward/step=4.01615\n",
      "iter=1590\tepsilon=0.050\treward/step=4.07348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:27:35,169] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 23:27:35,191] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1600\tepsilon=0.050\treward/step=4.12561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:27:36,579] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 5 timesteps with reward=76.0\n",
      "iter=1610\tepsilon=0.050\treward/step=4.18821\n",
      "iter=1620\tepsilon=0.050\treward/step=4.24571\n",
      "iter=1630\tepsilon=0.050\treward/step=4.30014\n",
      "iter=1640\tepsilon=0.050\treward/step=4.34538\n",
      "iter=1650\tepsilon=0.050\treward/step=4.39667\n",
      "iter=1660\tepsilon=0.050\treward/step=4.43855\n",
      "iter=1670\tepsilon=0.050\treward/step=4.47145\n",
      "iter=1680\tepsilon=0.050\treward/step=4.51231\n",
      "iter=1690\tepsilon=0.050\treward/step=4.55454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:34:42,548] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 23:34:42,594] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1700\tepsilon=0.050\treward/step=4.60729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:34:44,226] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps with reward=71.0\n",
      "iter=1710\tepsilon=0.050\treward/step=4.65872\n",
      "iter=1720\tepsilon=0.050\treward/step=4.70544\n",
      "iter=1730\tepsilon=0.050\treward/step=4.74600\n",
      "iter=1740\tepsilon=0.050\treward/step=4.79225\n",
      "iter=1750\tepsilon=0.050\treward/step=4.83604\n",
      "iter=1760\tepsilon=0.050\treward/step=4.86284\n",
      "iter=1770\tepsilon=0.050\treward/step=4.90550\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4ad940c2acd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nexes/anaconda2/lib/python2.7/site-packages/agentnet/experiments/openai_gym/pool.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n_steps, append, max_size, add_last_observation, preprocess)\u001b[0m\n\u001b[1;32m    204\u001b[0m             self.experience_replay.append_sessions(observation_tensor, action_tensor, reward_tensor,\n\u001b[1;32m    205\u001b[0m                                                    \u001b[0mis_alive_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreceding_memory_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                                                    max_pool_size=max_size or self.max_size)\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     def evaluate(self, n_games=1, save_path=\"./records\", use_monitor=True, record_video=True, verbose=True,\n",
      "\u001b[0;32m/Users/nexes/anaconda2/lib/python2.7/site-packages/agentnet/environment/session_pool.pyc\u001b[0m in \u001b[0;36mappend_sessions\u001b[0;34m(self, observation_sequences, action_sequences, reward_seq, is_alive, prev_memories, max_pool_size)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m#load everything into the environmnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sessions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_alive_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_memory_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nexes/anaconda2/lib/python2.7/site-packages/agentnet/environment/session_pool.pyc\u001b[0m in \u001b[0;36mload_sessions\u001b[0;34m(self, observation_sequences, action_sequences, reward_seq, is_alive, prev_memories)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobservation_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mset_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mset_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm_notebook #\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "\n",
    "for i in tqdm_notebook(range(2000)):  \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    loss = train_step()\n",
    "    \n",
    "    targetnet.load_weights(0.01)\n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = 0.05 + 0.95*np.exp(-epoch_counter/200.)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = pool.experience_replay.rewards.get_value().mean()\n",
    "        print(\"iter=%i\\tepsilon=%.3f\\treward/step=%.5f\"%(epoch_counter,\n",
    "                                                         current_epsilon,\n",
    "                                                         pool_mean_reward))\n",
    "        \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "        rewards[epoch_counter] = pool.evaluate(record_video=False)\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10c2dabd0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEACAYAAABGYoqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cd42dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time,rw = zip(*sorted(list(rewards.items()),key=lambda p:p[0]))\n",
    "plt.plot(time,map(np.mean,rw))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:41:53,056] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-19 23:41:53,087] Clearing 14 monitor files from previous run (because force=True was provided)\n",
      "[2017-03-19 23:41:54,566] Starting new video recorder writing to /Users/nexes/Documents/Study/Practical_RL/week4/DoomRecords/openaigym.video.18.41826.video000000.mp4\n",
      "[2017-03-19 23:41:54,999] Starting new video recorder writing to /Users/nexes/Documents/Study/Practical_RL/week4/DoomRecords/openaigym.video.18.41826.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 3 timesteps with reward=86.0\n",
      "Episode finished after 6 timesteps with reward=71.0\n",
      "Episode finished after 7 timesteps with reward=66.0\n",
      "Episode finished after 4 timesteps with reward=81.0\n",
      "Episode finished after 7 timesteps with reward=66.0\n",
      "Episode finished after 5 timesteps with reward=76.0\n",
      "Episode finished after 2 timesteps with reward=91.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:41:56,306] Starting new video recorder writing to /Users/nexes/Documents/Study/Practical_RL/week4/DoomRecords/openaigym.video.18.41826.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1 timesteps with reward=96.0\n",
      "Episode finished after 1 timesteps with reward=96.0\n",
      "Episode finished after 5 timesteps with reward=76.0\n",
      "Episode finished after 4 timesteps with reward=81.0\n",
      "Episode finished after 1 timesteps with reward=96.0\n",
      "Episode finished after 4 timesteps with reward=81.0\n",
      "Episode finished after 1 timesteps with reward=96.0\n",
      "Episode finished after 1 timesteps with reward=96.0\n",
      "Episode finished after 6 timesteps with reward=71.0\n",
      "Episode finished after 1 timesteps with reward=96.0\n",
      "Episode finished after 5 timesteps with reward=76.0\n",
      "Episode finished after 1 timesteps with reward=96.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:41:57,500] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nexes/Documents/Study/Practical_RL/week4/DoomRecords')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 5 timesteps with reward=76.0\n",
      "mean session score=83.500000.5\n"
     ]
    }
   ],
   "source": [
    "action_layer.epsilon.set_value(0.001)\n",
    "rw = pool.evaluate(n_games=20,save_path=\"./DoomRecords\",record_video=True)\n",
    "print(\"mean session score=%f.5\"%np.mean(rw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework II\n",
    "Get it work. We want stable positive score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus assignment II\n",
    "* Better env\n",
    "  * Switch to DoomDefendCenter, DoomHealthGathering, DoomDeathmatch __or__ any atari game you want.\n",
    "  * Try to get `better_than_random` score on any of those environments __(2+++ pts)__\n",
    "  * Deploy a better network. Doom will likely need some recurrent netsle\n",
    "     * Find an arcitecture which maxsimizes score __(bonus points depend on your ```mean_reward```)__  \n",
    "     * Bonus can get large as you approach state-of-the-art\n",
    "     \n",
    "* Deploy a different RL algorithm\n",
    "  * Try at least two RL algorithms which had been learned during the course and try to compare them on ```mean_reward``` with similar training time (**plot** or **table** would be good idea) __(3 pts)__\n",
    "  * See the note in assignment 4.1 on how to train on-policy\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "#with open(\"brains_doom.pcl\",'wb') as f:\n",
    "#    dump(get_all_param_values(qvalues_layer), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
